{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project, Part 1: *Proposal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draft a well-formed problem statement relevant to a business problem affecting your team, division, or organization.\n",
    "2. Include the following elements:\n",
    "   - Hypothesis/assumptions\n",
    "   - Goals and success metrics\n",
    "   - Risks or limitations\n",
    "3. Identify at least one relevant internal dataset and confirm that you have (or can get) the right access permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original dataset from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
    "- Kaggle dataset and competition from https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000/home\n",
    "- Scientific paper from https://arxiv.org/abs/1803.10417\n",
    "\n",
    "\n",
    "**null hypothesis:** There is no difference between dermatoscopic images of pigmented skin lesions between skin cancer diagnostic categories.\n",
    "\n",
    "**alternative hypothesis:** There is a difference between dermatoscopic images of pigmented skin lesions between skin cancer diagnostic categories.\n",
    "\n",
    "**goals and success:** Correctly classify dermatoscoptic images of pigmented skin lesions into a skin cancer diagnostic category at a probability higher than chance. There are 7 categories, so success metric would need to be 15% or greater.\n",
    "\n",
    "**risks or limitations:** Requires enough sample data in each diagnostic category. Also required enough similarities of images within diagnostic category as well as distinctions between diagnostic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project, Part 2: *Brief*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis is a crucial step in any data workflow. Create a Jupyter Notebook that explores your data mathematically and visually. Explore features, apply descriptive statistics, look at distributions, and determine how to handle sampling or any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an exploratory data analysis notebook.\n",
    "2. Perform statistical analysis, along with any visualizations.\n",
    "3. Determine how to handle sampling or missing values.\n",
    "4. Clearly identify shortcomings, assumptions, and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ham_df = pd.read_csv(\"./data/HAM10000_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ham_df.shape)\n",
    "display(ham_df.head())\n",
    "display(ham_df.describe(include=\"all\"))\n",
    "display(ham_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The HAM metadata has 10015 rows and 7 columns\n",
    "- There are 7470 lesions, however, some are taken with different magnification and angles resulting in 10,015 images accoridng to the paper. This serves as natural data augmentation\n",
    "- The 7 unique `dx` suggests we are classifying for multiple diseases and not binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values: \\n\", ham_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_null = ham_df[ham_df['age'].isnull()]\n",
    "age_null.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although there are NaN and unknown values for patient information. I propose we do not replace or drop values as I am considering not using them as features. These variables serve to show the distribution of data.\n",
    "- The feature would be the images themselves.\n",
    "- This is despite distribution of diagnoses varying with patient information.\n",
    "    - eg. melanoma on areas not exposed to sun (incidence higher in leg for women and back for men) as opposed to akiec (actinic keratoses on face and Bowen's disease in other areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Percentage distribution of target diagnostic values: \\n\", round(ham_df['dx'].value_counts(normalize=True)*100, 2))\n",
    "\n",
    "variables = ['dx', 'dx_type', 'age', 'sex', 'localization']\n",
    "for var in variables:\n",
    "    ham_df[var].value_counts().plot(kind=\"bar\")\n",
    "    plt.title(var)\n",
    "    plt.show();\n",
    "\n",
    "ham_df['age'].plot(kind=\"hist\", bins=15)\n",
    "plt.title(\"age\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dx`\n",
    "    - mostly nv (Melanocytic nevi) as these are benign/non-cancerous\n",
    "    - much less sample data for the others\n",
    "        - need to ensure there are enough sample data to include smaller categories\n",
    "        - when modelling need to consider techniques such as oversampling, stratified folds\n",
    "- `dx_type`\n",
    "    - will be accepting all forms of diagnosis as accurate and not just histogram\n",
    "- `localization`\n",
    "    - in reality, these would vary by `dx` and `sex`\n",
    "    \n",
    "- Target value is `dx_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dx_samples.png\" alt=\"Diagnostic category samples\" title=\"Diagnostic category samples\"/>\n",
    "- From the images we can see that there are visual differences within categories\n",
    "    - this is because they are categorised by biology\n",
    "    - for example, `vasc` includes angiomas, angioketaomas, pyogenic granulomas and haemorrahge and therefore presentations may vary\n",
    "    - in `blk`, a subset within (linchen-planus like keratoses) have features that are similar to `mel` and therefore ML may incorrectly classify these scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "- load and process data into `ham_df`\n",
    "- use `hmnist` file and/or learn how images can be processed into data\n",
    "- test train split\n",
    "- start with a simple model\n",
    "- read and find out what models are available and test these\n",
    "- explore errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project, Part 3: *Technical Notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a prototype model or process to successfully resolve the business problem you've chosen. Document your work in a technical notebook that can be shared with your peers.\n",
    "\n",
    "Build upon your earlier analysis, folling the performance metrics you established as part of your problem's evaluation criteria. Demonstrate your approach logically, including all relevant code and data. Polish your notebook for peer audiences by cleanly formatting sections, headers, and descriptions in markdown. Include comments in any code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A detailed Jupyter Notebook with a summary of your analysis, approach, and evaluation metrics.\n",
    "2. Clearly formatted structure with section headings and markdown descriptions.\n",
    "3. Comments explaining your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Organising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "ham_df = pd.read_csv(\"./data/HAM10000_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Kevin Mader for this code\n",
    "base_skin_dir = os.path.join(\".\", \"data\")\n",
    "\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(base_skin_dir, \"*\", \"*.jpg\"))}\n",
    "\n",
    "ham_df['path'] = ham_df['image_id'].map(imageid_path_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing label (diagnosis) into the name of image file\n",
    "for dx, filename in zip(ham_df['dx'], ham_df['path']):\n",
    "    path = filename[0:30]\n",
    "    image_id = filename[-12:]\n",
    "    os.rename(filename, path+dx+image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the image files are currently stored\n",
    "path_1 = \"./data/HAM10000_images_part_1\"\n",
    "path_2 = \"./data/HAM10000_images_part_2\"\n",
    "\n",
    "# Create a new directory to store all images together\n",
    "os.mkdir(path_img)\n",
    "\n",
    "# Function to copy files to this image directory\n",
    "def copy_files(path):\n",
    "    src_files = os.listdir(path)\n",
    "    for filename in src_files:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        if (os.path.isfile(full_filename)):\n",
    "            shutil.copy(full_filename, \"./data/images/\"+filename)\n",
    "   \n",
    "move_files(path_1)\n",
    "move_files(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory to store some images\n",
    "path_img = \"./data/images\"\n",
    "os.mkdir(path_test)\n",
    "\n",
    "# Function to move files to this test directory\n",
    "def move_files(dx, repeat):\n",
    "    src_files=os.listdir(path_img)\n",
    "    i = 0\n",
    "    dx = [filename for filename in src_files if dx in filename]\n",
    "    for i in range(repeat):\n",
    "        filename = dx[i]\n",
    "        full_filename = os.path.join(path_img, filename)\n",
    "        shutil.move(full_filename, \"./data/test_images/\"+filename)\n",
    "\n",
    "# Set aside these images for testing at the end\n",
    "move_files(\"akiec\", 3)\n",
    "move_files(\"bcc\", 2)\n",
    "move_files(\"bkl\", 2)\n",
    "move_files(\"df\", 2)\n",
    "move_files(\"mel\", 2)\n",
    "move_files(\"nv\", 2)\n",
    "move_files(\"vasc\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !curl https://course.fast.ai/setup/colab | bash\n",
    "%reload_ext_autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = \"./data/images/\"\n",
    "fnames = get_image_files(path_img)                  # grabs array of image files\n",
    "\n",
    "np.random.seed(42)                                  # sets same validation set\n",
    "\n",
    "# These are the data augmentations we'll be using (regularisation)\n",
    "tfms = get_transforms(flip_vert=True,               # 8 symmetric dihedral rots/flips\n",
    "                      max_rotate=20,                # rotate degrees\n",
    "                      max_zoom=1.1,                 # zoom\n",
    "                      max_lighting=0.3,             # lightin and contrast\n",
    "                      max_warp=0.3,                 # magnitude of warp\n",
    "                      p_affine=.9,                  # prob of applying affine and warp\n",
    "                      p_lighting=.5)                # prob of applying lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ImageItemList.from_folder(path_img).random_split_by_pct(0.2, seed=42)\n",
    "\n",
    "regex = r'([^/]+)_\\d+.jpg$'                         # regex\n",
    "pat = re.compile(regex)\n",
    "\n",
    "# We want to visualise these augmentations to ensure they look reasonable\n",
    "get_data = (src.label_from_re(regex)\n",
    "           .transform(tfms, size=128)\n",
    "           .databunch(bs=64).normalize(imagenet_stats))\n",
    "\n",
    "def _plot(i,j,ax):\n",
    "    x, y = get_data.train_ds[42]\n",
    "    x.show(ax, y=y)\n",
    "\n",
    "plot_multi(_plot, 3, 3, figsize=(9,9))\n",
    "plt.savefig(\"akiec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pat = re.compile(r'/([^/]+)_\\d+.jpg$')              \n",
    "# tfms = get_transforms(flip_vert=True,               # 8 symmetric dihedral rots/flips\n",
    "#                       max_lighting=0.1,             # lighting and contrast\n",
    "#                       max_zoom=1.05, \n",
    "#                       max_warp=0.15)                # perspective warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bunch contains train and validation\n",
    "data = ImageDataBunch.from_name_re(path_img,        # path with images\n",
    "                                   fnames,          # filenames\n",
    "                                   pat,             # regex\n",
    "                                   valid_pct=0.2,   # randomly sets aside 20% for validation\n",
    "                                   ds_tfms=tfms,    # data augmentation\n",
    "                                   size=128,        # image size\n",
    "                                   bs=64)           # value depends on gpu\n",
    "data.normalize(imagenet_stats)                      # pixel values of RGB to have mean 0 and std 1\n",
    "\n",
    "# print(\"With our split, there are {0} train images and {1} validation images.\"\n",
    "#       .format(len(data.train_ds), len(data.valid_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes                                        # there are 7 classes (labels)\n",
    "data.show_batch(rows=4, figsize=(9,9))              # show some contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convolutional neural net with pretrained weights from imagenet\n",
    "learn = create_cnn(data, \n",
    "                   models.resnet50,                 # architecture\n",
    "                   metrics=error_rate)              # (1 - accuracy) against val set\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()                                     # learning rate finder\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2                                           # using biggest downward slope\n",
    "learn.fit_one_cycle(4, slice(lr)\n",
    "                   pct_start=0.8) ### try this\n",
    "\n",
    "# epoch\ttrain_loss\tvalid_loss\terror_rate\n",
    "# 1\t0.939380\t0.783771\t0.268597\n",
    "# 2\t0.706262\t0.607229\t0.220669\n",
    "# 3\t0.572653\t0.538121\t0.198702\n",
    "# 4\t0.509552\t0.520804\t0.197204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-1-resnet50-128\")                  # save weights\n",
    "learn.unfreeze()                                    # unfreeze so we can train the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()                               # before it goes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, slice(1e-5, lr/5))\n",
    "\n",
    "# epoch\ttrain_loss\tvalid_loss\terror_rate\n",
    "# 1\t0.533032\t0.521723\t0.198203\n",
    "# 2\t0.432744\t0.449853\t0.168248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-2-resnet50-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above with larger image size - progressive resizing\n",
    "data = ImageDataBunch.from_name_re(path_img, \n",
    "                                   fnames, \n",
    "                                   pat, \n",
    "                                   valid_pct=0.2, \n",
    "                                   ds_tfms=tfms, \n",
    "                                   size=256,        # using larger image size\n",
    "                                   bs=64)\n",
    "data.normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data\n",
    "learn.freeze()                                      # freeze model\n",
    "\n",
    "data.train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "learn.fit_one_cycle(5, slice(lr))\n",
    "\n",
    "# epoch\ttrain_loss\tvalid_loss\terror_rate\n",
    "# 1\t0.473655\t0.396796\t0.151273\n",
    "# 2\t0.425479\t0.375579\t0.137793\n",
    "# 3\t0.408397\t0.358457\t0.130804\n",
    "# 4\t0.369705\t0.349039\t0.126311\n",
    "# 5\t0.347600\t0.345409\t0.126311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-1-resnet50-256\")\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(1e-5, lr/10))\n",
    "\n",
    "# epoch\ttrain_loss\tvalid_loss\terror_rate\n",
    "# 1\t0.350378\t0.340163\t0.126311\n",
    "# 2\t0.330948\t0.321495\t0.118822\n",
    "# 3\t0.281673\t0.295341\t0.102846\n",
    "\n",
    "# epoch\ttrain_loss\tvalid_loss\terror_rate\n",
    "# 1\t0.361077\t0.345955\t0.124813\n",
    "# 2\t0.325243\t0.313263\t0.114329\n",
    "# 3\t0.284516\t0.304413\t0.109835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-2-resnet50-256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above with larger image size - progressive resizing\n",
    "data = ImageDataBunch.from_name_re(path_img, \n",
    "                                   fnames, \n",
    "                                   pat, \n",
    "                                   valid_pct=0.2, \n",
    "                                   ds_tfms=tfms, \n",
    "                                   size=512,        # using larger image size\n",
    "                                   bs=64)\n",
    "data.normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data\n",
    "learn.freeze()\n",
    "\n",
    "data.train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-3\n",
    "# learn.fit_one_cycle(5, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-1-resnet50-512\")\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(3, slice(1e-5, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-2-resnet50-512\")\n",
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Predict top losses\n",
    "interp.plot_top_losses(9, figsize=(15,11))\n",
    "plt.savefig(\"pred_act_loss_prob.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and actual that was wrong most often\n",
    "interp.most_confused(min_val=2) \n",
    "\n",
    "# Confusion matrix\n",
    "interp.plot_confusion_matrix(figsize=(9,9)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves all information for inference eg. transforms, model weights\n",
    "learn.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
