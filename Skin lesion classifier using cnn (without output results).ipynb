{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project, Part 1: *Proposal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draft a well-formed problem statement relevant to a business problem affecting your team, division, or organization.\n",
    "2. Include the following elements:\n",
    "   - Hypothesis/assumptions\n",
    "   - Goals and success metrics\n",
    "   - Risks or limitations\n",
    "3. Identify at least one relevant internal dataset and confirm that you have (or can get) the right access permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original dataset from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
    "- Kaggle dataset and competition from https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000/home\n",
    "- Scientific paper from https://arxiv.org/abs/1803.10417\n",
    "\n",
    "\n",
    "**null hypothesis:** There is no difference between dermatoscopic images of pigmented skin lesions between skin cancer diagnostic categories.\n",
    "\n",
    "**alternative hypothesis:** There is a difference between dermatoscopic images of pigmented skin lesions between skin cancer diagnostic categories.\n",
    "\n",
    "**goals and success:** Correctly classify dermatoscoptic images of pigmented skin lesions into a skin cancer diagnostic category at a probability higher than chance. There are 7 categories, so success metric would need to be 15% or greater.\n",
    "\n",
    "**risks or limitations:** Requires enough sample data in each diagnostic category. Also required enough similarities of images within diagnostic category as well as distinctions between diagnostic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project, Part 2: *Brief*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis is a crucial step in any data workflow. Create a Jupyter Notebook that explores your data mathematically and visually. Explore features, apply descriptive statistics, look at distributions, and determine how to handle sampling or any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an exploratory data analysis notebook.\n",
    "2. Perform statistical analysis, along with any visualizations.\n",
    "3. Determine how to handle sampling or missing values.\n",
    "4. Clearly identify shortcomings, assumptions, and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ham_df = pd.read_csv(\"./data/HAM10000_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ham_df.shape)\n",
    "display(ham_df.head())\n",
    "display(ham_df.describe(include=\"all\"))\n",
    "display(ham_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The HAM metadata has 10015 rows and 7 columns\n",
    "- There are 7470 lesions, however, some are taken with different magnification and angles resulting in 10,015 images accoridng to the paper. This serves as natural data augmentation\n",
    "- The 7 unique `dx` suggests we are classifying for multiple diseases and not binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values: \\n\", ham_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_null = ham_df[ham_df['age'].isnull()]\n",
    "age_null.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although there are NaN and unknown values for patient information. I propose we do not replace or drop values as I am considering not using them as features. These variables serve to show the distribution of data.\n",
    "- The feature would be the images themselves.\n",
    "- This is despite distribution of diagnoses varying with patient information.\n",
    "    - eg. melanoma on areas not exposed to sun (incidence higher in leg for women and back for men) as opposed to akiec (actinic keratoses on face and Bowen's disease in other areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Percentage distribution of target diagnostic values: \\n\", round(ham_df['dx'].value_counts(normalize=True)*100, 2))\n",
    "\n",
    "variables = ['dx', 'dx_type', 'age', 'sex', 'localization']\n",
    "for var in variables:\n",
    "    ham_df[var].value_counts().plot(kind=\"bar\")\n",
    "    plt.title(var)\n",
    "    plt.show();\n",
    "\n",
    "ham_df['age'].plot(kind=\"hist\", bins=15)\n",
    "plt.title(\"age\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dx`\n",
    "    - mostly nv (Melanocytic nevi) as these are benign/non-cancerous\n",
    "    - much less sample data for the others\n",
    "        - need to ensure there are enough sample data to include smaller categories\n",
    "        - when modelling need to consider techniques such as oversampling, stratified folds\n",
    "- `dx_type`\n",
    "    - will be accepting all forms of diagnosis as accurate and not just histogram\n",
    "- `localization`\n",
    "    - in reality, these would vary by `dx` and `sex`\n",
    "    \n",
    "- Target value is `dx_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dx_samples.png\" alt=\"Diagnostic category samples\" title=\"Diagnostic category samples\"/>\n",
    "- From the images we can see that there are visual differences within categories\n",
    "    - this is because they are categorised by biology\n",
    "    - for example, `vasc` includes angiomas, angioketaomas, pyogenic granulomas and haemorrahge and therefore presentations may vary\n",
    "    - in `blk`, a subset within (linchen-planus like keratoses) have features that are similar to `mel` and therefore ML may incorrectly classify these scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "- load and process data into `ham_df`\n",
    "- use `hmnist` file and/or learn how images can be processed into data\n",
    "- test train split\n",
    "- start with a simple model\n",
    "- read and find out what models are available and test these\n",
    "- explore errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project, Part 3: *Technical Notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a prototype model or process to successfully resolve the business problem you've chosen. Document your work in a technical notebook that can be shared with your peers.\n",
    "\n",
    "Build upon your earlier analysis, folling the performance metrics you established as part of your problem's evaluation criteria. Demonstrate your approach logically, including all relevant code and data. Polish your notebook for peer audiences by cleanly formatting sections, headers, and descriptions in markdown. Include comments in any code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A detailed Jupyter Notebook with a summary of your analysis, approach, and evaluation metrics.\n",
    "2. Clearly formatted structure with section headings and markdown descriptions.\n",
    "3. Comments explaining your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Organising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "ham_df = pd.read_csv(\"./data/HAM10000_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Kevin Mader for this code\n",
    "base_skin_dir = os.path.join(\".\", \"data\")\n",
    "\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(base_skin_dir, \"*\", \"*.jpg\"))}\n",
    "\n",
    "ham_df['path'] = ham_df['image_id'].map(imageid_path_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing label (diagnosis) into the name of image file\n",
    "for dx, filename in zip(ham_df['dx'], ham_df['path']):\n",
    "    path = filename[0:30]\n",
    "    image_id = filename[-12:]\n",
    "    os.rename(filename, path+dx+image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Renaming 10,015 image files with the diagnosis in the file name. It is not only heplful for later, but it helps me understand what, if any, each diagnoses of skin lesions look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the image files are currently stored\n",
    "path_1 = \"./data/HAM10000_images_part_1\"\n",
    "path_2 = \"./data/HAM10000_images_part_2\"\n",
    "\n",
    "# Create a new directory to store all images together\n",
    "# os.mkdir(\"./data/images/\")\n",
    "\n",
    "# Function to copy files to this image directory\n",
    "def copy_files(path):\n",
    "    src_files = os.listdir(path)\n",
    "    for filename in src_files:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        if (os.path.isfile(full_filename)):\n",
    "            shutil.copy(full_filename, \"./data/images/\"+filename)\n",
    "   \n",
    "copy_files(path_1)\n",
    "copy_files(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory to store some images\n",
    "path_img = \"./data/images\"\n",
    "os.mkdir(\"./data/test_images/\")\n",
    "\n",
    "# Function to move files to this test directory\n",
    "def move_files(dx, repeat):\n",
    "    src_files=os.listdir(path_img)\n",
    "    i = 0\n",
    "    dx = [filename for filename in src_files if dx in filename]\n",
    "    for i in range(repeat):\n",
    "        filename = dx[i]\n",
    "        full_filename = os.path.join(path_img, filename)\n",
    "        shutil.move(full_filename, \"./data/test_images/\"+filename)\n",
    "\n",
    "# Set aside these images for testing at the end\n",
    "move_files(\"akiec\", 3)\n",
    "move_files(\"bcc\", 2)\n",
    "move_files(\"bkl\", 2)\n",
    "move_files(\"df\", 2)\n",
    "move_files(\"mel\", 2)\n",
    "move_files(\"nv\", 2)\n",
    "move_files(\"vasc\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have placed all images in one folder for train & validation\n",
    "- From train & validation folder, I have removed 15 images to test at the very end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !curl https://course.fast.ai/setup/colab | bash\n",
    "%reload_ext_autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = \"./data/images/\"\n",
    "fnames = get_image_files(path_img)                  # grabs array of image files\n",
    "\n",
    "np.random.seed(42)                                  # sets same validation set\n",
    "\n",
    "# These are the data augmentations we'll be using (regularisation)\n",
    "tfms = get_transforms(flip_vert=True,               # 8 symmetric dihedral rots/flips\n",
    "                      max_rotate=20,                # rotate degrees\n",
    "                      max_zoom=1.1,                 # zoom\n",
    "                      max_lighting=0.3,             # lightin and contrast\n",
    "                      max_warp=0.3,                 # magnitude of warp\n",
    "                      p_affine=.9,                  # prob of applying affine and warp\n",
    "                      p_lighting=.5)                # prob of applying lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ImageItemList.from_folder(path_img).random_split_by_pct(0.2, seed=42)\n",
    "\n",
    "regex = r'([^/]+)_\\d+.jpg$'                         # regex\n",
    "pat = re.compile(regex)\n",
    "\n",
    "# We want to visualise these augmentations to ensure they look reasonable\n",
    "get_data = (src.label_from_re(regex)\n",
    "           .transform(tfms, size=128)\n",
    "           .databunch(bs=64).normalize(imagenet_stats))\n",
    "\n",
    "def _plot(i,j,ax):\n",
    "    x, y = get_data.train_ds[211]\n",
    "    x.show(ax, y=y)\n",
    "\n",
    "plot_multi(_plot, 3, 3, figsize=(9,9))\n",
    "plt.savefig(\"akiec_augmentations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"akiec_augmentations.png\" alt=\"Actinic keratosis augmentations\" title=\"Actinic keratosis augmentations\"/>\n",
    "- I am visualising to ensure the data augmentations parameters that I passed are appropriate\n",
    "- Data augmentation is an important step in computer vision by passing in a lot more data\n",
    "- Nb: fast.ai applies `padding_mode = \"reflection\"` as default on otherwise black borders generated by data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data bunch contains train and validation\n",
    "data = ImageDataBunch.from_name_re(path_img,        # path with images\n",
    "                                   fnames,          # filenames\n",
    "                                   pat,             # regex\n",
    "                                   valid_pct=0.2,   # randomly sets aside 20% for validation\n",
    "                                   ds_tfms=tfms,    # data augmentation\n",
    "                                   size=128,        # image size\n",
    "                                   bs=64)           # value depends on gpu\n",
    "data.normalize(imagenet_stats)                      # pixel values of RGB to have mean 0 and std 1\n",
    "\n",
    "# print(\"With our split, there are {0} train images and {1} validation images.\"\n",
    "#       .format(len(data.train_ds), len(data.valid_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e22ad79bfc26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m                                        \u001b[0;31m# there are 7 classes (labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# show some contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.classes                                        # there are 7 classes (labels)\n",
    "data.show_batch(rows=4, figsize=(9,9))              # show some contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convolutional neural net with pretrained weights from imagenet\n",
    "learn = create_cnn(data, \n",
    "                   models.resnet50,                 # architecture\n",
    "                   metrics=accuracy)                # against val set\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will be using a convulational neural network, using the resnet50 (50 layers) architecture and the 2015 winner of imagenet\n",
    "- Our goal is the improve accuracy\n",
    "- We will be looking at the learning rate plot to select a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()                                     # learning rate finder\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2/2                                         # using largest downward slope\n",
    "learn.fit_one_cycle(15, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-1-resnet50-128\")                  # save weights\n",
    "learn.unfreeze()                                    # unfreeze so we can train the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()                               # before it goes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, slice(1e-4/2, lr/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting the correct epoch and learning rate will ensure the model is not over and under-fitting\n",
    "- If we are happy with the train loss, valid loss and accuracy then we can save the model (in case we want to reload it)\n",
    "- Next we will repeat with progressive resizing of images. This avoids overfitting and improves generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-2-resnet50-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above with larger image size - progressive resizing\n",
    "data = ImageDataBunch.from_name_re(path_img, \n",
    "                                   fnames, \n",
    "                                   pat, \n",
    "                                   valid_pct=0.2, \n",
    "                                   ds_tfms=tfms, \n",
    "                                   size=256,        # using larger image size\n",
    "                                   bs=64)\n",
    "data.normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data\n",
    "learn.freeze()                                      # freeze model\n",
    "\n",
    "data.train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2/2\n",
    "learn.fit_one_cycle(10, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-1-resnet50-256\")\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(7, slice(1e-5, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage-2-resnet50-256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model has a 93% accuracy against the validation set\n",
    "- We are happy with this result as it has surpassed our initial goals and dispproven the null hypothesis\n",
    "- We could continue progressive resizing, however, we will need to decrease batch size. The subsequent gain in accuracy will be minimal and it will take a lot longer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "\n",
    "# Predict top losses\n",
    "interp.plot_top_losses(9, figsize=(15,11))\n",
    "plt.savefig(\"pred_act_loss_prob.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and actual that was wrong most often\n",
    "display(interp.most_confused(min_val=2))\n",
    "\n",
    "# Confusion matrix\n",
    "interp.plot_confusion_matrix(figsize=(9,9)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pred_act_loss_prob.png\" alt=\"Prediction Actual Loss Probability\" title=\"Prediction Actual Loss Probability\"/>\n",
    "- These are our incorrect predictions\n",
    "- We can use these to visualise and see if they are incorrect or misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusion_matrix.png\" alt=\"Confusion Matrix\" title=\"Confusion Matrix\"/>\n",
    "- Most of the predictions fall on the diagonal axis (correctly predicted), however, we can also see where predictions may go wrong by looking at the darker boxes (eg. between mel and blk)\n",
    "- We are happy with the confusion matrix, as the incorrect predictions are fairly evenly distributed on the whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves all information for inference eg. transforms, model weights\n",
    "learn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you to the fast.ai lessons taught by Jeremy Howard. His lessons on deep learning can be found at https://course.fast.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
